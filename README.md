# FastAPI Reflect

A PoC project using [pydantic-ai](https://github.com/pydantic/pydantic-ai/tree/main) to demonstrate an API capable of self-reflection
by using LLM models. This project is a simple experiment and is not intended for
production use, nor is it particular configurable.

## Getting Started


### Prerequisites

* Python >= 3.12
* [`uv`](https://github.com/astral-sh/uv)
* An OpenAI API Key
* [Logfire](https://pydantic.dev/logfire) (Optional)
* [`just`](https://github.com/casey/just) (Optional)

### Running

Use `dev` mode to get live reloading and extra development features. Use `run`
for a cleaner experience.

With `just`, you can start the app using:

```shell
just [dev|run]
```

Without, you can run using `uv` and the `fastapi` CLI:

```shell
uv run --env-file .env -- fastapi [dev|run] ./src/fastapi_reflect/main.py
```

Once running, open the API at `http://127.0.0.1:8000/docs` to see interactive docs
in the browser. 

## Concept

The goal of this PoC is to create a web API that can inspect itself to generate
real, functional queries that can be used to interact with the system.

The project is a simple FastAPI with two routes:

* `/songs` - Mock "song" data endpoints
* `/ai` - Endpoints for interacting with the LLM model

In a production setting, the `/songs` route would contain normal business data
that would be exposed to customers, while the `/ai` route provides operations for
introspection.

The user provides a plaintext query to the system, such as the following:

> Generate a request to create a new random song.
> Generate a request to retrieve an existing song from the database.

and the LLM will return a `curl` command that implements the user's request. The
command should be completely valid.

The LLM is augmented with a limited tool to retrieve existing data from the system.

## Future Work

This concept will not replace bespoke or even auto-generated API clients. However,
it can be used to facilitate API exploration via plaintext, rather than needing
to dive into documentation or code. It can also potentially speed up development
of custom clients or query code for users that would prefer to write their own
rather than use a generated SDK.

Some potential areas for improvement:

* Expand the mock API to test the LLM's capabilities in parsing more complex
  requirements such as filtering, sorting, or paginating.
* Introduce data constraints (e.g. uniqueness) to test the LLM's reasoning
* Introduce query validation to check commands generated by the LLM
* Augment the agent with more tools as needed
